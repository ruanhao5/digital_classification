{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0+cu101\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os, sys, glob, shutil, json\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from tensorboardX import SummaryWriter   # tensorboard\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVHNDataset(Dataset):\n",
    "    def __init__(self, img_path, img_label, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.img_label = img_label \n",
    "        if transform is not None:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.img_path[index]).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # 原始SVHN中类别10为填充的数字X, 最多字符为6\n",
    "        lbl = np.array(self.img_label[index], dtype=np.int)\n",
    "        lbl = list(lbl)  + (4 - len(lbl)) * [10]\n",
    "        \n",
    "        return img, torch.from_numpy(np.array(lbl[:4]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "\n",
    "train_path = glob.glob('../input/mchar_train/*.png')\n",
    "train_path.sort()\n",
    "train_json = json.load(open('../input/mchar_train.json'))\n",
    "train_label = [train_json[x]['label'] for x in train_json]\n",
    "\n",
    "val_path = glob.glob('../input/mchar_val/*.png')\n",
    "val_path.sort()\n",
    "val_json = json.load(open('../input/mchar_val.json'))\n",
    "val_label = [val_json[x]['label'] for x in val_json]\n",
    "\n",
    "# BATCH_SIZE = 10\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        SVHNDataset(train_path, train_label,\n",
    "                   transforms.Compose([\n",
    "                       transforms.Resize((64, 128)),\n",
    "                       transforms.ColorJitter(0.3, 0.3, 0.2),\n",
    "                       transforms.RandomRotation(5),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])), \n",
    "    batch_size=BATCH_SIZE, # 每批样本个数\n",
    "    shuffle=True, # 是否打乱顺序\n",
    "    num_workers=10, # 读取的线程个数\n",
    ")\n",
    "\n",
    "    \n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    SVHNDataset(val_path, val_label,\n",
    "                    transforms.Compose([\n",
    "                       transforms.Resize((64, 128)),\n",
    "                    #    transforms.ColorJitter(0.3, 0.3, 0.2),\n",
    "                    #    transforms.RandomRotation(5),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=10, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# 定义模型\n",
    "class SVHN_Model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SVHN_Model1, self).__init__()\n",
    "        # CNN提取特征模块\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2)),\n",
    "            nn.ReLU(),  \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2)),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # \n",
    "        self.fc1 = nn.Linear(32*3*7, 11)\n",
    "        self.fc2 = nn.Linear(32*3*7, 11)\n",
    "        self.fc3 = nn.Linear(32*3*7, 11)\n",
    "        self.fc4 = nn.Linear(32*3*7, 11)\n",
    "    \n",
    "    def forward(self, img):        \n",
    "        feat = self.cnn(img)\n",
    "        feat = feat.view(feat.shape[0], -1)\n",
    "        c1 = self.fc1(feat)\n",
    "        c2 = self.fc2(feat)\n",
    "        c3 = self.fc3(feat)\n",
    "        c4 = self.fc4(feat)\n",
    "        return c1, c2, c3, c4\n",
    "\n",
    "# model = SVHN_Model1()\n",
    "model = SVHN_Model1().cuda()\n",
    "# 损失函数\n",
    "criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
    "best_loss = 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    # 切换模型为训练模式\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "\n",
    "    for (data, label) in tqdm(train_loader):\n",
    "        data = data.cuda()\n",
    "        label = label.cuda()\n",
    "        c0, c1, c2, c3 = model(data)\n",
    "        loss = criterion(c0, label[:, 0]) + \\\n",
    "                criterion(c1, label[:, 1]) + \\\n",
    "                criterion(c2, label[:, 2]) + \\\n",
    "                criterion(c3, label[:, 3])\n",
    "        loss /= (4*BATCH_SIZE)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    # 切换模型为预测模型\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_predict = []\n",
    "\n",
    "    # 不记录模型梯度信息\n",
    "    with torch.no_grad():\n",
    "        for (data, label) in tqdm(val_loader):\n",
    "            data = data.cuda()\n",
    "            label = label.cuda()\n",
    "            c0, c1, c2, c3 = model(data)\n",
    "            # print(c0.shape)\n",
    "            loss = criterion(c0, label[:, 0]) + \\\n",
    "                    criterion(c1, label[:, 1]) + \\\n",
    "                    criterion(c2, label[:, 2]) + \\\n",
    "                    criterion(c3, label[:, 3]) \n",
    "            loss /= (4*BATCH_SIZE)\n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            # calculate validation accuracy\n",
    "            output = np.concatenate([\n",
    "                c0.data.cpu().numpy(),\n",
    "                c1.data.cpu().numpy(),\n",
    "                c2.data.cpu().numpy(),\n",
    "                c3.data.cpu().numpy()], axis=1)# output: shape(batch, 44)\n",
    "            val_predict.append(output)        \n",
    "        val_predict = np.vstack(val_predict)\n",
    "        \n",
    "        val_predict = np.vstack([\n",
    "            val_predict[:, :11].argmax(1),\n",
    "            val_predict[:, 11:22].argmax(1),\n",
    "            val_predict[:, 22:33].argmax(1),\n",
    "            val_predict[:, 33:44].argmax(1)]).T\n",
    "        val_predict_label = list()\n",
    "        for x in val_predict:\n",
    "            val_predict_label.append(''.join(map(str, x[x!=10])))\n",
    "        val_label = [''.join(map(str, x)) for x in val_loader.dataset.img_label]\n",
    "        val_acc = np.mean(np.array(val_predict_label) == np.array(val_label))\n",
    "    return np.mean(val_loss), val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.59it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.57it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.80it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.62it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.69it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.57it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.76it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.70it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.79it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.64it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.75it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.67it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.70it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.63it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.73it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.68it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.66it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.66it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.85it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.67it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.78it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.66it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.73it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.66it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.78it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.55it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.74it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.62it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.74it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.43it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.62it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.63it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.65it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.58it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  17\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "train_loss_plot = list()\n",
    "val_loss_plot = list()\n",
    "val_acc_plot = list()\n",
    "\n",
    "# writer = SummaryWriter('log')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "for epoch in range(20):\n",
    "    print('\\nEpoch: ', epoch)\n",
    "\n",
    "    train_loss = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    val_loss, val_acc = validate(val_loader, model, criterion)# val_predict: shape(10000, 44)\n",
    "\n",
    "    \n",
    "    # 记录下验证集精度\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), './model.pt')\n",
    "    \n",
    "    train_loss_plot.append(train_loss)\n",
    "    val_loss_plot.append(val_loss)\n",
    "    val_acc_plot.append(val_acc)\n",
    "    writer.add_scalar('Loss/train', float(train_loss), epoch)\n",
    "    writer.add_scalar('Loss/validation', float(val_loss), epoch)\n",
    "    writer.add_scalar('val_acc', float(val_acc), epoch)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print(\"\\nruning time:\", datetime.now() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_plot)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"train_loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(val_loss_plot)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"val_loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(val_acc_plot)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"val_acc\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\ntrain_loss:\", train_loss_plot)\n",
    "print(\"\\nval_loss:\", val_loss_plot)\n",
    "print(\"\\nval_acc:\", val_acc_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3b9b828f383373597d353e22ddc5577ec47150a1819a90192bb3706a239b968"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
